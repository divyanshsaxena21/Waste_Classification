{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyOxSlTmoJaG3i6ClDX6mP0l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/divyanshsaxena21/Waste_Classification/blob/main/Waste.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umY3cOOI88pI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16353b0f-fbc8-4a0a-98b0-e923e8fcd751"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/sumn2u/garbage-classification-v2\n",
            "License(s): MIT\n",
            "Downloading garbage-classification-v2.zip to /content\n",
            " 98% 729M/744M [00:04<00:00, 154MB/s]\n",
            "100% 744M/744M [00:04<00:00, 156MB/s]\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download sumn2u/garbage-classification-v2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile('/content/garbage-classification-v2.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content')"
      ],
      "metadata": {
        "id": "X0LWEdBa936A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kiU58DIC970Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "cDclX3IO-Fvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q4px54VS-GUC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "dataset_dir = '/content/garbage-dataset'  # Path to your dataset\n",
        "train_dir = '/content/train'  # Path for the training set\n",
        "test_dir = '/content/test'  # Path for the testing set\n",
        "test_size = 0.2  # Proportion of data for testing (e.g., 0.2 for 20%)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "FXshZJHZ-GsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "8PbvvaOV-ILR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "for class_name in os.listdir(dataset_dir):\n",
        "    class_dir = os.path.join(dataset_dir, class_name)\n",
        "    images = os.listdir(class_dir)\n",
        "    train_images, test_images = train_test_split(images, test_size=test_size, random_state=42)  # Use random_state for reproducibility\n",
        "\n",
        "    # Move images to respective directories\n",
        "    for image in train_images:\n",
        "        src_path = os.path.join(class_dir, image)\n",
        "        dst_path = os.path.join(train_dir, class_name, image)\n",
        "        os.makedirs(os.path.dirname(dst_path), exist_ok=True)\n",
        "        shutil.copy(src_path, dst_path)\n",
        "\n",
        "    for image in test_images:\n",
        "        src_path = os.path.join(class_dir, image)\n",
        "        dst_path = os.path.join(test_dir, class_name, image)\n",
        "        os.makedirs(os.path.dirname(dst_path), exist_ok=True)\n",
        "        shutil.copy(src_path, dst_path)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "hqsgqzDy-I_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "metadata": {
        "id": "nLnDNS-5-JLA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the CNN model\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    layers.Flatten(),\n",
        "\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "\n",
        "    layers.Dense(10, activation='softmax')  # 10 categories\n",
        "])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ChKY04j-xYZ",
        "outputId": "5f4bc8fd-1d76-41b7-b63d-d0bc750e6401"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define image size\n",
        "img_size = (128, 128)\n",
        "\n",
        "# Prepare data generators\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,  # Normalize images\n",
        "    rotation_range=20,  # Random rotation\n",
        "    width_shift_range=0.2,  # Horizontal shift\n",
        "    height_shift_range=0.2,  # Vertical shift\n",
        "    shear_range=0.2,  # Shearing\n",
        "    zoom_range=0.2,  # Zoom\n",
        "    horizontal_flip=True,  # Flip images randomly\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)  # Normalize images\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    '/content/train',  # Correct path to train data\n",
        "    target_size=img_size,\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "    '/content/test',  # Correct path to test data\n",
        "    target_size=img_size,\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9cfblkE-sFQ",
        "outputId": "4e402641-23ab-4189-d3c2-fb8d9af86d66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 15806 images belonging to 10 classes.\n",
            "Found 3956 images belonging to 10 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training class indices:\", train_generator.class_indices)\n",
        "print(\"Number of training samples:\", train_generator.samples)\n",
        "print(\"Number of test samples:\", validation_generator.samples)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqmTS1MXAE_b",
        "outputId": "7ae58479-49bb-4c84-e1e2-3083777647dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training class indices: {'battery': 0, 'biological': 1, 'cardboard': 2, 'clothes': 3, 'glass': 4, 'metal': 5, 'paper': 6, 'plastic': 7, 'shoes': 8, 'trash': 9}\n",
            "Number of training samples: 15806\n",
            "Number of test samples: 3956\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_batch, y_batch = next(train_generator)\n",
        "print(x_batch.shape)  # Should print (batch_size, img_height, img_width, 3)\n",
        "print(y_batch.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kevGIY4kAOrH",
        "outputId": "35ff5354-cde3-4b06-951f-f622b92f91d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32, 128, 128, 3)\n",
            "(32, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "Ss9C8htE-9Wy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
        "    epochs=20,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=validation_generator.samples // validation_generator.batch_size\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VAsJtRM4_BP6",
        "outputId": "47801452-29ef-4330-a868-da56da6625ff"
      },
      "execution_count": 7,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m493/493\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m464s\u001b[0m 935ms/step - accuracy: 0.3303 - loss: 1.9621 - val_accuracy: 0.4822 - val_loss: 1.5293\n",
            "Epoch 2/20\n",
            "\u001b[1m493/493\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 275us/step - accuracy: 0.4688 - loss: 1.4565 - val_accuracy: 0.5500 - val_loss: 1.2711\n",
            "Epoch 3/20\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.11/contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self.gen.throw(typ, value, traceback)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m493/493\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m455s\u001b[0m 921ms/step - accuracy: 0.4445 - loss: 1.6285 - val_accuracy: 0.5396 - val_loss: 1.3151\n",
            "Epoch 4/20\n",
            "\u001b[1m493/493\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 245us/step - accuracy: 0.4375 - loss: 1.4384 - val_accuracy: 0.5500 - val_loss: 1.1811\n",
            "Epoch 5/20\n",
            "\u001b[1m493/493\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m455s\u001b[0m 920ms/step - accuracy: 0.4919 - loss: 1.4951 - val_accuracy: 0.5376 - val_loss: 1.3100\n",
            "Epoch 6/20\n",
            "\u001b[1m493/493\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 224us/step - accuracy: 0.5938 - loss: 1.1744 - val_accuracy: 0.6000 - val_loss: 1.3025\n",
            "Epoch 7/20\n",
            "\u001b[1m493/493\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m454s\u001b[0m 918ms/step - accuracy: 0.5158 - loss: 1.4191 - val_accuracy: 0.5279 - val_loss: 1.3915\n",
            "Epoch 8/20\n",
            "\u001b[1m493/493\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 383us/step - accuracy: 0.5938 - loss: 1.4530 - val_accuracy: 0.7000 - val_loss: 0.8091\n",
            "Epoch 9/20\n",
            "\u001b[1m493/493\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m454s\u001b[0m 918ms/step - accuracy: 0.5338 - loss: 1.3654 - val_accuracy: 0.6118 - val_loss: 1.1139\n",
            "Epoch 10/20\n",
            "\u001b[1m493/493\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 214us/step - accuracy: 0.4375 - loss: 1.7249 - val_accuracy: 0.7000 - val_loss: 0.8319\n",
            "Epoch 11/20\n",
            "\u001b[1m493/493\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m457s\u001b[0m 924ms/step - accuracy: 0.5500 - loss: 1.3013 - val_accuracy: 0.6369 - val_loss: 1.0976\n",
            "Epoch 12/20\n",
            "\u001b[1m493/493\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 238us/step - accuracy: 0.7500 - loss: 0.8410 - val_accuracy: 0.6500 - val_loss: 1.0141\n",
            "Epoch 13/20\n",
            "\u001b[1m493/493\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m459s\u001b[0m 927ms/step - accuracy: 0.5784 - loss: 1.2527 - val_accuracy: 0.6146 - val_loss: 1.1433\n",
            "Epoch 14/20\n",
            "\u001b[1m493/493\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 425us/step - accuracy: 0.5938 - loss: 1.3516 - val_accuracy: 0.7500 - val_loss: 0.5788\n",
            "Epoch 15/20\n",
            "\u001b[1m493/493\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m460s\u001b[0m 930ms/step - accuracy: 0.5857 - loss: 1.2323 - val_accuracy: 0.6448 - val_loss: 1.0799\n",
            "Epoch 16/20\n",
            "\u001b[1m493/493\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 248us/step - accuracy: 0.7812 - loss: 0.9311 - val_accuracy: 0.7000 - val_loss: 0.6440\n",
            "Epoch 17/20\n",
            "\u001b[1m493/493\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m460s\u001b[0m 931ms/step - accuracy: 0.5973 - loss: 1.1874 - val_accuracy: 0.6583 - val_loss: 1.0206\n",
            "Epoch 18/20\n",
            "\u001b[1m493/493\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 236us/step - accuracy: 0.5625 - loss: 1.1967 - val_accuracy: 0.7000 - val_loss: 0.8967\n",
            "Epoch 19/20\n",
            "\u001b[1m493/493\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m457s\u001b[0m 925ms/step - accuracy: 0.6099 - loss: 1.1454 - val_accuracy: 0.6593 - val_loss: 1.0624\n",
            "Epoch 20/20\n",
            "\u001b[1m493/493\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 239us/step - accuracy: 0.5938 - loss: 1.2049 - val_accuracy: 0.5500 - val_loss: 1.3773\n"
          ]
        }
      ]
    },
    {
      "source": [
        "# Evaluate the model on the test set\n",
        "test_loss, test_acc = model.evaluate(validation_generator, steps=validation_generator.samples // validation_generator.batch_size)\n",
        "print('Test accuracy:', test_acc)\n",
        "print('Test loss:', test_loss)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9pdIO8JSseY",
        "outputId": "d130ef82-a9d2-4d5d-9819-7f92130b5d60"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 225ms/step - accuracy: 0.6663 - loss: 1.0360\n",
            "Test accuracy: 0.658790647983551\n",
            "Test loss: 1.0662403106689453\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eZaz4vsFAVqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from tensorflow.keras.preprocessing import image\n",
        "import numpy as np\n",
        "\n",
        "# Load and preprocess a new image\n",
        "img_path = input(\"Enter path: \\n\")  # Path to your new image\n",
        "img = image.load_img(img_path, target_size=(128, 128))\n",
        "img_array = image.img_to_array(img)\n",
        "img_array = np.expand_dims(img_array, axis=0)\n",
        "img_array /= 255.0  # Normalize\n",
        "\n",
        "# Make a prediction\n",
        "prediction = model.predict(img_array)\n",
        "predicted_class_index = np.argmax(prediction)\n",
        "\n",
        "# Get the class labels\n",
        "class_labels = list(train_generator.class_indices.keys())  # Get class labels from the generator\n",
        "\n",
        "# Print the predicted class\n",
        "print(\"Predicted class:\", class_labels[predicted_class_index])"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "juhPW-T5S2W9",
        "outputId": "4b8f3153-0c2e-44e7-c445-18f402a8fb20"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter path: \n",
            "/content/test/battery/battery_122.jpg\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "Predicted class: battery\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing import image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display, Image\n",
        "\n",
        "# Load and preprocess a new image\n",
        "img_path = input(\"Enter image path: \\n\")  # Path to your new image\n",
        "img = image.load_img(img_path, target_size=(128, 128))\n",
        "img_array = image.img_to_array(img)\n",
        "img_array = np.expand_dims(img_array, axis=0)\n",
        "img_array /= 255.0  # Normalize\n",
        "\n",
        "# Make a prediction\n",
        "prediction = model.predict(img_array)\n",
        "predicted_class_index = np.argmax(prediction)\n",
        "\n",
        "# Get the class labels\n",
        "class_labels = list(train_generator.class_indices.keys())  # Get class labels from the generator\n",
        "recyclability_dict = {\n",
        "    \"cardboard\": \"recyclable\",\n",
        "    \"glass\": \"recyclable\",\n",
        "    \"metal\": \"recyclable\",\n",
        "    \"paper\": \"recyclable\",\n",
        "    \"plastic\": \"recyclable\",\n",
        "    \"battery\": \"non-recyclable\",\n",
        "    \"biological\": \"non-recyclable\",  # Often compostable, but not typically recyclable\n",
        "    \"clothes\": \"non-recyclable\",  # May be reusable or donated\n",
        "    \"e-waste\": \"non-recyclable\", # Special recycling programs may exist\n",
        "    \"medical\": \"non-recyclable\",\n",
        "    \"plastic bag\": \"non-recyclable\",  # Some stores have bag recycling programs\n",
        "    \"shoes\": \"non-recyclable\"  # May be reusable or donated\n",
        "}\n",
        "# Print the predicted class\n",
        "print(\"Predicted class:\", class_labels[predicted_class_index])\n",
        "\n",
        "# Get recyclability from the dictionary\n",
        "recyclability = recyclability_dict.get(class_labels[predicted_class_index])\n",
        "\n",
        "# Print recyclability\n",
        "print(\"This waste is\", recyclability)\n",
        "# Display the image using IPython.display.Image\n",
        "display(Image(filename=img_path))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "EhploJJBS4qR",
        "outputId": "b4c9e29b-e41e-49b1-e2ca-834d0bfc323e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter image path: \n",
            "/content/test/battery/battery_1.jpg\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "Predicted class: battery\n",
            "This waste is non-recyclable\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/jpeg": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Fv5oWT_ZUqoV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}